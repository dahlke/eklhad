[
 {
  "id": "https://spreadsheets.google.com/feeds/list/1Ex7AuwS25FoyP_h_HYVQjKr3XwPurZd2Heos3zb2gBI/o3dkf67/public/values/cokwr",
  "name": "Setting up Sublime Text 3 from the Command Line",
  "timestamp": 1385164800,
  "url": "https://gist.github.com/dahlke/7330342",
  "path": "2013-11-23-setting-up-sublime-text-3-from-the-command-line",
  "content": "# [2013-11-23] Setting up Sublime Text 3 from the Command Line\n\nTo make Sublime Text available via the command line argument `subl` use:\n\n```\nsudo ln -s \"/Applications/Sublime Text.app/Contents/SharedSupport/bin/subl\" /usr/bin/subl\n```\n\nThen to use your editor as the default for git commits and such, enter:\n\n```\ngit config --global core.editor \"sub --wait\"\n```"
 },
 {
  "id": "https://spreadsheets.google.com/feeds/list/1Ex7AuwS25FoyP_h_HYVQjKr3XwPurZd2Heos3zb2gBI/o3dkf67/public/values/cpzh4",
  "name": "MemSQL, Tableau, and the Democratization of Data",
  "timestamp": 1478736000,
  "url": "http://blog.memsql.com/memsql-tableau-and-the-democratization-of-data/",
  "path": "2016-11-10-memsql-tableau-and-the-democritization-of-data",
  "content": "# MemSQL, Tableau, and the Democratization of Data\n_Originally published on the [MemSQL Blog](http://blog.memsql.com/memsql-tableau-and-the-democratization-of-data/)._\n\n_[We love fast databases. It makes the experience of interacting with your database that much more enjoyable.” – Tableau](https://www.tableau.com/about/blog/2016/8/tableau-10-includes-even-more-data-source-options-57505)_\n\nToday’s business decisions are about seconds, not minutes. To accommodate this trend, businesses have moved to evidence-backed decision making and widespread data access. Modern business intelligence tools abound, making it easier for the average analyst to create compelling visualizations. In this post, I’ll address how this new mode of thinking about data, the Democratization of Data, comes with two challenges – making data easily available and making it actionable in real time.\n\n---\n\n### Making Data Available\nCompanies are migrating to a new model of data distribution – shared access to a centralized database with both historical data and real-time data. This is a far cry from the traditional approach of using many small database instances with stale data, isolated silos, and limited user access. Now, raw data is available to everyone. Employees are empowered to dive into the data, discover new opportunities, or close efficiency gaps in a way that has never been done before. The need for data now coupled with scalability has attracted many developers to in-memory, clustered databases.\n\n### Making Data Actionable in Real Time\nInnovations in data visualization have produced powerful, usable tools that afford companies the opportunity to be data-driven. One tool we see embedded across different industries is Tableau. With its mature ecosystem and rich featureset, the business intelligence platform makes it easy for individuals to create compelling, interactive data visualizations. It is a very attractive package for different business levels because it does not require expertise or a degree in visual design or information systems. Any user can create meaningful, actionable dashboards providing views of the business from thirty thousand feet as well as at ground level.\n\nBut even with a Tableau license in hand, users still face issues – the dashboards are slow or the data is stale. The problem often lies in the database layer. It is imperative that data is up-to-date to be relevant to today’s fast moving business operations. Common issues include:\n\n![Long Query Time](https://storage.googleapis.com/eklhad-web-public/images/query-time.gif)\n\n- No access to real-time, limited to batch loads\n- Slow query execution\n- Concurrency limitations\n- Siloed data\n- Limited hardware choices / technical debt accumulation\n\nMemSQL and Tableau have collaborated to provide the best possible business intelligence solution on the market, making great strides to address the aforementioned challenges. In August, Tableau announced the named [MemSQL Connector](https://help.tableau.com/current/pro/desktop/en-us/examples_memsql.htm), available in Tableau 10, for native integration between the two platforms.\n\nAt Tableau Conference 2016, MemSQL showcases MemSQL Springs, a real-time showcase application for resort demographic analysis. The story is simple. When a team of executives gathers in a room and ends up staring at a loading spinner, the company bleeds money. You do not have multiple minutes to wait for your dashboards to update each and every time you want to ask your data a question.\n\n![MemSQL Springs](https://storage.googleapis.com/eklhad-web-public/images/memsql-springs.gif)\n\nAnd yet this is an issue that many organizations still face. Slow dashboards are a roadblock to becoming data-driven. Your business needs instant results.\n\nBy combining MemSQL and Tableau, users have reduced query latency from many minutes to a few seconds. This puts time back on the clock and dollars back in businesses’ pockets."
 },
 {
  "id": "https://spreadsheets.google.com/feeds/list/1Ex7AuwS25FoyP_h_HYVQjKr3XwPurZd2Heos3zb2gBI/o3dkf67/public/values/cre1l",
  "name": "jQuery Conference Austin",
  "timestamp": 1378771200,
  "url": "https://gist.github.com/dahlke/6509891",
  "path": "2013-09-10-jquery-conference-austin",
  "content": "# [2013-09-10] jQuery Conference 2013: Austin\n\n##### 10 Sept 2013\n\n---\n\n## The State of jQuery - Dave Methvin\n\n- Bower and NPM dependency management out next month.\n- Finding forced layouts (http://jsfiddle.net/qjncp)\n- Full page forced layouts occur every time you ask the browser if we have enough content during an infinite scroll. That's a lot of work on the browser.\n- Alternative: Instead of asking the browser every time, determine the height of a row, and track how long we've scrolled. Ex: 200px high, the page scrolled 740px, we need 4 more rows.\n- When looking for elements, using class names such as `.hidden` will provide faster selection than `:hidden` as it doesn't need to check style rules or snything else, it only has to inspect the DOM tree. It won't force a layout, works well with styling, and is battery efficient.\n- Throttle high frequency events like mousemove or scroll; handle with requestAnimationFrame (http://www.html5rocks.com/en/tutorials/speed/rendering/)\n- Moral: Know your tools. Modern browsers have the tools to find these issues. (Even IE... 11)\n\n---\n\n## Digging and Debugging - Brian Arnold\n\n#### Non Technical Tips\n\n- Read Javascript: The Definitive Guide. Not just The Good Parts.\n- Verbally express what the problem is, even if you don't expect help from that person. It helps to give you clarity on the problem.\n\n#### Understand the Tools\n- Chrome is the most feature rich debugger.\n- `$0` is an alias in the Chrome command line to the most recently inspected DOM node. When you select something else, it becomes \"$1\" and so on.\n- `$_` declares a variable for the last command so that we can use it and inspect it's DOM structure easily. This is useful in comparison to `$('label')` which is an invocation, which we cannot use command line autocomplete on.\n- `console.group()` - allows you to group multiple log statements so that you can expand and contract them.\n- Using `dir($0)` allows you to view any element as more of an object which makes it a bit easier to read.\n- `keys(jQuery)` will give you an array of all the keys in the command line.\n- `copy($_)` gives you a string based representation of the last object and copies it to your clipboard. You can pass this through `JSON.stringify()` and get a JSON representation of that data.\n- Using the debugger gives you great access to the console executing in the proper context.\n- You can set _conditional breakpoints_ in Chrome debugger so that you only stop if a given condition is met.\n- You can also set breakpoints on the DOM, so for example, you can set it such that you will get a break when any subtree element is modified.\n- XHR breakpoints can be set when the URL contains a given string as well.\n- Using pretty print can help to make minified source a bit more readable.\n- Using profiles, we can record a bit of time on the page, and get a detailed result on processing. This gives you a very clear visual as to where your time is going.\n\n\n#### Techniques\n\n- Private Mode - no history, cache, cookies.\n- Use a style guide. Look into things like [Idiomatic](https://github.com/rwaldron/idiomatic.js/) and [EditorConfig](http://editorconfig.org/).\n- `console.count(\"functionName\")` will show you how many times a function is invoked.\n- Which code is mine? It's often an `anonymous function` as it is often used as a callback in a library.\n\n#### Resources\n- Discover DevTools\n- CommandLine API (Chrome)\n- Console API (Chrome)\n- Command Line API (Firebug)\n\n\n---\n\n## Getting the Most out of jQuery Widgets - Richard Lindsey\n\n- Think small. Think modular. Elements, cells, compounds, organisms.\n- Keep them decoupled. Subscribe and Respond. Communicate through events.\n- Observe and mediate. Bundle smaller moduls, provide a public api, direct references only go downward, each layer consumes lower-level events and publishes upwards.\n- Make it testable. Does it perform operation or calculation? Is it part of the widget's public facing API?\n- Public functions should have unit tests, store prototypes in object namespaces, test logical functions separately.\n- *Summary:* _Only make componenets as large as they need to be. Keep them as decoupled as possible. Consume downwards, communicate upwards. Make functions and options granular and robust for potential overrides. Test, test, and test. Make every attempt to ensure backward compatibility._\n- Presentation slides can be found at: http://bit.ly/jqwidgets\n\n---\n\n## Using jQuery Mobile to Build Device Agnostic Pages - Asta Gindulyte\n\n- http://pubchem.ncbi.nlm.nih.gov/\n- Challenges: screen-size, touch, content organization, and testing.\n- Screen size variety challenge - no scroll, and font size big enough to read.\n- Touch challenge - buttons big enough to touch, swipe and other gestures should be intuitive.\n- Content organization challegne - showing/hiding based on screen size (sometimes people with small screens seem penalized.). Having diffferent layouts from large to small screen may confuse users.\n- Device testing challenge - no free lunch, you really need to test on all devices to make sure everything is working as expected. Emulators like [screenfly](http://quirktools.com/screenfly/) can help.\n- Why jQuery Mobile? Cross browser, cross device, touch friendly, responsive, layout and theming engine, ajax page navigation and _great documentation_.\n\n---\n\n## Grunt Automates All of the Things ... What's Next? - Aaron Stacy\n\n- Don't just build, *ship*.\n- _Releases should not be \"tribal knowledge\"._ If one of your teammates was going to get hit by a bus, could you still push a release?\n- Continuous integration - everything you do when you deploy, except all the time, every commit.\n- [TravisCI](https://travis-ci.org/) as an alternative to [Jenkins](http://jenkins-ci.org/)\n\n##### Continuous integration example.\n[Source](https://github.com/aaronj1335/shipit)\n\n- make `grunt test` work. ([Selenium](http://docs.seleniumhq.org/) and [PhantomJS](http://phantomjs.org) recommended)\n  - `grunt-contib-qunit`\n  - grunt plugins for mocha\n  - phantom.js\n  - saucelabs\n- `.travis.yml`\n- Sign up for Travis with GitHub, then flip the toggle for the repo we want to be testing.\n- We want to;\n  - run tests on every commit.\n  - receive an e-mail when something fails.\n  - _not_ receive an e-mail when something doesn't fail.\n  - know if merging a pull request will cause problems.\n\n##### Another example\n- make `grunt deploy` work\n  - heroku\n  - nodejitsu\n  - openshift\n  - github pages (example grunt task)\n  - The key to these three is making a deploy as simple as git push.\n  - GitHub API token\n  - Secure variables for Travis\n\n---\n\n## Simply Pushing the Web Forward - Kris Borchers\n\n[Slides](http://pushtalk-aerogearkb.rhcloud.com/#/), [AeroGear](https://github.com/aerogear)\n\n- APNS (Apple Push Notification Service [IOS, Safari])\n- GCM (Google Cloud Messaging [Droid])\n- MPNS (Microsoft Push Notification Service [Windows 8])\n- BlackBerry Push\n- SimplePush (Firefox OS / Android / Desktop, and more)\n- Push API (W3C) and SimplePush (Mozilla)\n  - Push API  is complicated, SimplePush is not.\n- Aerogear acts as a middleware for all of these different specs. (Plugs for [OpenShift](https://www.openshift.com/) and [UnifiedPush](http://aerogear.org/docs/guides/aerogear-push-ios/unified-push-server/))\n\n---\n\n## Creating 3D Worlds with voxel.js and jQuery - Vlad Flippov\n\n[Site](http://voxeljs.com/)\n\n- [ArchiveRoom](http://archiveroom.net/)\n- [VoxelBuilder](http://voxelbuilder.com/)\n- [VoxelDrone](https://github.com/shama/voxel-drone)\n\n---\n\n## How to How-to\n##### Or, Tips for Effectively Educating New Developers\n\n- _At one time, we all knew nothing about our jobs. And that's easy to forget. Learning new things is a crucial part of doing your job!_\n- Why be a teacher? It enhances your leadership skills, has a positive influence on other developers careers, and the rest of the dev team will thank you.\n\n#### Making the Most out of Teaching \u0026 Learning Styles\n- Recognition of distinct learning preferences allows you to customize the teaching process.\n- Two Common Learning Styles\n  - Creative / Visual\n    - A top-down/holistic approach can be effective.\n    - Use examples and demonstartions.\n    - Show where you intend to end up, and then get more granular.\n  - Logical\n    - Effective learning begins with understanding the *most basic elements* of the subject.\n    - Problem solving is linear. If this, then that.\n    - Effective learning happens step-by-step.\n    - Avoid advanced concepts until the basics are down pat.\n  - Match your teaching style with the developers learning style.\n- Getting a sense for learning styles - ask questions!\n  - How do you learn best?\n  - Tell be when you struggled to learn things?\n- The best predictor of future behavior, is past behavior!\n- Be inspirational!\n  - Encourage curiousity.\n  - Have empathy.\n  - Encourage an optimistic attitude.\n  - Show respect.\n- The best learning happens when solving real problems.\n  - Get new devs in the traenches!\n- Best times?\n  - Pair Programming\n  - Code Reviews\n  - Stupid Questions Sessions\n- What topics should I prioritize when teaching new developers?\n  - Critical thinking\n    - Teach understanding of *concepts* behind any technology used.\n  - Separating Concerns\n    - Don't mix structure, style, and functionality.\n  - Resourcefulness\n    - Get new devs using community-developed tools.\n    - Small victories - the daily ego boosters. For new devs, make it as easy as possible to achieve small victories.\n  - Code reusability\n    - Encourage reusable CSS classes and abstracted JS functionality.\n  - Debugging (tools as well)\n\n_New devs are eager to learn and build. More experience developers can either be intimidating or inspiring. *Be inspiring.*_\nA new dev's best weapon is his own curiousity. Hiring *passionate, curious* developers goes a long way in the learning process.\n\n---\n\n## AngularJS Directives and the Computer Science of Javascript - Burke Holland\n\n[Slides](https://presentboldly.com/burkeholland/angularjs-and-the-computer-science-of-javascript)\n\n- Imperative vs. Declarative\n  - Markup should describe the behaviour and configuration of the UI. HTML is not adequate.\n  - Declaritive - Leverage HTML attributes to specify configuration.\n  - ViewModels, allows for declarative initialization and two way binding. Has a bad rap.\n  - $scope and Controllers\n    - $scope - The View Model\n    - Controller - The context in which you work with the $scope object. The $scope is injected into the Controller by AngularJS.\n    - Some popular directives:\n      - Linking\n      - Restriction\n      - Template and Replace\n      - Model Binding\n      - Passing Config\n      - Transclusion and Timeout\n\n---\n\n## Kiss My Canvas: Making and Facillitating Art with Code - Jenn Schiffer\n\n- Example: Creating a simple basketball.\n- Example: Creating a `canvas` that we can draw on.\n\n---\n\n##### 11 Sept 2013\n\n---\n\n## jQuery UI \u0026 Mobile Keynote (The Great Merger) - Scott González\n\n- The touch events model is so fragmented on the web that it is rather difficult to implement touch events in jQuery UI\n- jQuery UI is meant to be code that works everywhere.\n  - Large and small devices\n  - Fast and slow connections\n  - Assistive tech\n  - Known and unknown environments\n- CSS Framework\n  - Page layout\n  - Responsive design\n    - Responsive Grid added in jQuery Mobile, they are working on more\n  - New icons\n    - Defaults to SVG + external PNG. Unoptimized css, no config, works everywhere\n    - Opt-In for better performance, which requires config\n  - Useful with and without JavaScript\n  - Simpler/Cleaner markup and CSS\n  - Fewer DOM manipulations\n  - Better performance\n  - Useful for prototyping like Bootstrap\n  - Shared between both projects\n  - Preferable shared with other JS libraries\n- Widgets that work everywhere\n  - Widget Factory - Common API \u0026 Extensibility\n  - Pointer Events - Interaction Abstraction\n  - Responsive Design\n- Dependency Management\n  - AMD / UMD for all jQuery projects\n  - Bower for all jQuery projects\n  - Please only load what you need\n- Web Components\n  - Investigating how to transition\n  - Making sure the spec solves real probles\n  - Not quite there yet, if you're interested, check out [Polymer](http://www.polymer-project.org/polymer.html)\n\n---\n\n## In Search of Front-End Modularity - Juan Pablo Buritica\n[Slides](https://speakerdeck.com/buritica/in-search-of-front-end-modularity) - Thanks [Juan](https://github.com/Buritica)!\n\n- As our applications evolve, complexity will increase. Function, libraries, frameworks, and architectures.\n- We want to build complext systems, without knowing their final state.\n- Be pragmatic. Nature solves problems by breaking larger systems into modular components.\n- Functional Elements\n  - Reusable\n  - Isolated\n  - Self-contained\n  - Promote separation of concerns\n  - Allow composition\n  - Standardized interfaces\n  - Communication (optional)\n- Benefits\n  - Scalability\n  - Structure\n  - Ease of change\n  - Testability\n  - Lower cost\n  - Flexibility\n  - Augmentation\n- Front-End modularity has an enemy, the code is not executed where it lives.\n- We need to deliver the smallest size, in the least amount of requests.\n- Modular Front-End Architectures\n  - Everything is a component\n  - Small Core\n  - Base (DOM, Utils, etc.)\n  - Messaging via events]\n- Web Components\n  - Templates\n  - Decorators\n    - Do not have a spec yet\n    - Are meant to enhance or override presentation of an existing element\n  - Custom Elements\n    - Allow author definition of DOM elements\n    - Access to prototype via nested \u003cscript\u003e\n    - Element lifecycle callbacks\n    - Available via markup and scripts\n  - Shadow DOM\n    - Provides encapsulation and enables composition\n    - The meat of web components\n    - Complext topic, read [spec](http://www.w3.org/TR/shadow-dom/) for further ifo\n  - HTML Imports\n  - Other stuff (mutation observers, pointer events, data binding)\n\n---\n\n## AMD-ifying jQuery Source: Game, Consider Yourself Changed - Timmy Willison\n\n[Slides](http://timmywil.github.io/jquery-amdify/)\n\n- AMD: Asynchronous Module Definition\n- You can now include only the modules that you will need during development, which will include it's dependencies\n- AMD + Building\n  - Reduce network requests\n  - Reduce size\n  - Flexible, pliable builds\n- Future Roadmap\n  - jQuery Mobile and UI to list core depencies\n  - Further fragmentation\n  - Grunt task for building jQuery\n\n---\n\n## Stop Procrastinating and Start Deferring - Julian Aubourg\n\n[Gist of the Talk](https://gist.github.com/jaubourg/6525351)\n\n---\n\n## Building a Development Culture - Monika Piotrowicz\n\n- Process - a process to support creativity, not resist it.\n- Web devs should be part of the design process from the very beginning, they should not just be handed down the plans from the designers and PMs.\n- The process must support learning, taking time during the week to research.\n- We should work _with_ designers.\n- Design and Development should always be occuring at the same time, not one before the other.\n- Developer and Designer should sit together and work out the project together. It levels the playing field, and allows for a more collaborative approach to design.\n- Research \u0026 Prototype - the chance to actually learn about and implement the new things that interest you.\n- Prototypes are your sandbox - the freedom to focus on one problem at a time, testing early to solve it faster.\n- The earlier an assumption is challenged, the easier it will be to fix.\n- Devs also will get the chance to drive the design, as you can show what's possible within the code, while meeting the design specifications.\n- Share \u0026 Iterate, design informs development.\n- Devs in the company\n  - Demo days to show off what everyone has done within the last chunk of time\n  - Meetings and lightning talks, keeping up to date with what the other developers have been up to / learned\n  - Hack Days \u0026 Recess - exploration and experimentation (about once per quarter for 2 or so days at a time) where they can collaborate with new people and try out new technologies.\n- What _they_ gain\n  - Make more informed decisions\n  - Time is reduced and quality increases\n  - Happy developers\n- What can _we_ do?\n  - Talk to your designers - your ally\n  - Become an advocate - don't just vent, have meaningful discussions\n  - Take the first step, even if it's on your own time, the results will speak for themselves\n\n---\n\n## jQuery Mobile: Optimizing Performance - Alex Schmitz\n\n- On average, mobile is about 3x slower.\n- Simplify your pages\n  - Reduce the widgets in each page\n  - Reduce the size of your pages\n  - Limit the size of your lists and tables\n  - Use pagination\n- To reduce download time and http requests\n  - combine all scripts\n  - combine all CSS\n  - minify all scripts and css\n  - do not include scripts in the page\n  - consider multi-page template\n- Multi-page template\n  - load all pages with single request\n  - reduces # of http requests\n  - faster page load\n  - slower intial page download\n  - leads to a large DOM\n  - uses more system memory\n  - cannot load multi-page via ajax\n  - not good for large # of pages\n- Custom builds\n  - reduce file size\n  - reduce library init time\n  - reduce page init time\n  - remove parts of library you aren't using\n\n---\n\n## jQuery + Phantom.js + Node.js = Testing and Automation Awesomesauce! - Travis Tidwell\n\n[Slides](http://travistidwell.com/presentations/jquery-node-phantom) - [Source](https://github.com/travist/presentations)\n\n- Not many great jQuery ports for Phantom\n- Serial\n  - waits for one operation to end before moving on to the next.\n  - gets interesting when they depend on one another\n  - and even more interesting when `getSomething` depends on an asychronous source.\n  - this leads to a mess of nested code\n- You can use [async.js](https://github.com/caolan/async) which will allow for a more serial type of programming, without having to nest all of your functions. (Not _too_ disimilar to `$.Deferred`, based on JavaScript promises.)\n- Leveraging node.js, using jQuery.go, [nconf](https://github.com/flatiron/nconf), and [prompt](https://github.com/flatiron/prompt).\n\n---\n\n## Journey to the Center of jQuery - Anne-Gaelle Colom\n\n- Interesting anecdotes on moving from GitHub committer to member of the jQuery team.\n- Bits on how to contribute to jQuery.\n\n---\n\n## Talk To Me: Making websites accessible - Jörn Zaefferer\n\n- Specifically, for people who can't see well.\n- Convince your boss that accessibility is important, like user experience and security\n  - Boosts your user experience - accessible websites are more usable for everyone\n  - Support powerusers - keyboard access for everyone\n  - More customers - when you can't leave home, shopping online is so much more useful\n  - It's the law\n  - SEO improvements\n- How to test\n  - W3C Validator\n  - Keyboard testing\n    - Put away the mouse, use tab key to navigate, try links, buttons, forms\n- Virtual Cursor - going from one node to the next\n- Headers, Links, Forms, and Landmarks (The GOTO of screenreaders)\n  - You can specify landmarks with `role`s.\n- WAI-ARIA (Web Accessibility Initiative) Accessible Rich Internet Applications\n  - Screenreader testing\n- Ask for forgiveness, not permission. Convince the boss. Test keyboard and screen reader. Fix by hand or with frameworks.\n\n---\n\n## Adaptive Images for Responsive Web Design - Christopher Schmitt\n\n- Feature testing vs. Browser Sniffing\n  - Browser width\n  - Screen resolution\n  - Bandwidth\n- Speed tests hinder speed and user experience\n  - Native speed tests on the way (in Android only now)\n- IMG\n  - .htaccess\n  - `\u003cpicture\u003e` and/or `srcset` attribute\n  - HiSRC\n    - Does a native speed test\n    - Checks screen density\n- Workarounds\n  - background-size: auto;\n    - [fittext](http://fittextjs.com/)\n  - SVG\n    - Use online compression tools as Illustrator generates massive SVGs\n  - font-based solutions\n    -  icon fonts\n      -  [fontello](http://fontello.com/)\n      -  [icomoon](http://icomoon.io/)"
 },
 {
  "id": "https://spreadsheets.google.com/feeds/list/1Ex7AuwS25FoyP_h_HYVQjKr3XwPurZd2Heos3zb2gBI/o3dkf67/public/values/chk2m",
  "name": "WhoJS Tutorial",
  "timestamp": 1376006400,
  "url": "https://gist.github.com/dahlke/7ce2ea3e476c04aa7ef7",
  "path": "2013-08-09-whojs-tutorial",
  "content": "# [2013-08-09] WhoJS Tutorial\n_[WhoJS README](https://github.com/dahlke/whojs/blob/master/README.md)_\n\nIt is important to note, that while using WhoJS, it is assumed that all HTML forms are marked up properly, including `label` tags having the proper `for` attribute pointed at their corresponding `input`, `select`, `textarea` or what have you. If the form is not marked up properly it is highly probable that Who will be unable to locate the correct input, despite having some small builtin fallbacks.\n\n## Contents\n\n1. Getting Started\n2. Using WhoJS\n\t- Setting Context\n\t- Reset Context\n\t- Typing Values Into a Form\n\t- Reading Values from a Form\n\t- Clicking Elements\n\t- Passing Time\n\n## Getting Started\nGrab yourself a copy of WhoJS from [GitHub](https://github.com/globusonline/whojs) and include it in your page similar to this:\n```\n\u003cscript src=\"javascripts/who.js\"\u003e \u003c/script\u003e\n```\n\n#### Setting Context\n\nSetting context is useful for zoning in on a specific part of the page Who is looking at. Running this example will show you very quickly a shift in Who's context. You can see what part of the page Who is actively looking at by finding the blue highlighted area.\n\n```\nWho.setContext('#new_context_example');\n```\n#### Reset Context\nSometimes you want Who to be watching over the whole page. A quick reset of context can be achieved using `resetContext()`. You'll notice when you run this code that the context is shifted back to the `body`.\n\n```\nWho.resetContext();\n```\n\n#### Typing Values Into a Form\n\nWhen a `Who` user wants to type a value into a form, there are two things they need. The text of the `\u003clabel\u003e` they are looking for, and the `input`, which is the text that will be put into the field.\n\n```\nWho.typesValueIntoForm({\n    label : \"Name\",\n    input : \"Neil Dahlke\"\n});\n```\n\nThe labels are case insensitive, and can also be search by partial matches using the `findByPartial` flag.\n```\nWho.typesValueIntoForm({\n    label : \"Tail\",\n    input : \"Shark Fin\",\n    findByPartial : true\n});\n```\n\n#### Clicking on elements\n##### Clicking links\n\n```\nWho.setContext('#click_example');\n\nWho.clicksElement({\n\ttext : \"Click Me!\",\n\ttype : \"a\"\n});\n```\nor you can specify links using the `LINK` keyword.\n```\nWho.setContext('#click_example');\n\nWho.clicksElement({\n\ttext : \"Click\",\n\ttype : \"LINK\",\n\tfindByPartial : true\n});\n```\n##### Clicking buttons\n##### Clicking other elements\n\n#### Reading form values\n```\nWho.typesValueIntoForm({\n\tlabel : \"my name\",\n\tinput : \"J Dilla\",\n\tfindByPartial : true\n});\n\nvar formVal = Who.readsFormValue({\n\tlabel : \"My NaMe Is:\"\n});\n\nalert('Hi '+(formVal||\"Who\")+'!');\n```\n\n\n#### Seeing if elements are on screen\n\n\n#### Passing time\nWhen you need Who to chill for a moment, maybe while you're fetching some things with AJAX, you can use `passesTime` with the option `thenWho` as a required callback for when Who finishes waiting.\n\n```\nvar _start = new Date();\n\nWho.passesTime({\n\tms : 2000,\n\tthenWho : function() {\n\t\tvar _end = new Date(),\n\t\t\t_waited = ((_end - _start)/1000);\n\t\talert(\"Who waited \"+_waited+\" seconds!\");\n\t}\n});\n```"
 },
 {
  "id": "https://spreadsheets.google.com/feeds/list/1Ex7AuwS25FoyP_h_HYVQjKr3XwPurZd2Heos3zb2gBI/o3dkf67/public/values/ciyn3",
  "name": "The Internet of Furbies",
  "timestamp": 1412121600,
  "url": "http://blog.memsql.com/the-internet-of-furbies/",
  "path": "2014-10-01-the-internet-of-furbies",
  "content": "# [2014-10-01] The Internet of Furbies\n_Originally published on the [MemSQL Blog](http://blog.memsql.com/the-internet-of-furbies/)._\n\n![The Furbies Themselves](https://storage.googleapis.com/eklhad-web-public/images/the-internet-of-furbies.png)\n\nAt MemSQL engineering there are few things we love as much as building great products. One, however, is a good laugh. This past weekend a team from MemSQL set out to make our mothers proud at [Cultivated Wit](https://www.cultivatedwit.com/)’s [Comedy Hack Day](http://www.comedyhackday.org/), an event for comedians and hackers to get together in attempt to make the funniest hack. And drink whiskey.\n\n---\n\nI’m very excited to share with you our product today, the grand prize winner, and the future of wearable technology. Engineering at MemSQL goes to great lengths to ensure that our technology enhances your life, rather than detracting from your special moments. Our engineering and design teams have determined that the best place for strapping technology to your body is not to your face or to your wrist, but rather, to your shoulder.\n\nThe wait is over, the next furry thing is here. We present to you: AwwCog – the future of wearable technology.\n\n[![Wearable Furby: Comedy Hack Day Five Grand Prize Winner](https://img.youtube.com/vi/iB3Cuu2ZWq8/0.jpg)](https://www.youtube.com/watch?v=iB3Cuu2ZWq8)\n\nIt comes in _the perfect size for everyone_. Isn’t it beautiful? Wait a second, then look again. It’s still beautiful, isn’t it? It’s even programmable in python! Since the original Furby was invented, nothing has been this revolutionary. We’re awesome. We know.\n\nIf you’re an engineer that likes to laugh as much as you like to code, we want to talk to you. Reach out to us at recruiting@memsql.com, and let us know who you are, and what you like to build.\n\nFor the technical folk that want to build their own AwwCogs, or contribute to the code, the repos can be found at:\n\n- [Furback](https://github.com/carlsverre/Furback)\n- [open-furby-platform](https://github.com/dahlke/hackathon-open-furby-platform)\n"
 },
 {
  "id": "https://spreadsheets.google.com/feeds/list/1Ex7AuwS25FoyP_h_HYVQjKr3XwPurZd2Heos3zb2gBI/o3dkf67/public/values/ckd7g",
  "name": "How Pinterest Measures Real-Time User Engagement with Spark",
  "timestamp": 1424217600,
  "url": "http://blog.memsql.com/pinterest-apache-spark-use-case/",
  "path": "2015-06-17-how-pinterest-measures-real-time-user-engagement-with-spark",
  "content": "# [2015-06-17] How Pinterest Measures Real-Time User Engagement with Spark\n_Originally published on the [MemSQL Blog](http://blog.memsql.com/pinterest-apache-spark-use-case/)._\n\n![Spark Demo Animated](https://storage.googleapis.com/eklhad-web-public/images/spark-demo-animated.gif)\n\n### Setting the Stage for Spark\n\nWith Spark [on track to replace MapReduce](https://www.lightbend.com/company/news/survey-indicates-apache-spark-gaining-developer-adoption-as-big-datas-projects-require-processing-speed), enterprises are flocking to the open source framework in effort to take advantage of its superior distributed data processing power.\n\nIT leads that manage infrastructure and data pipelines of high-traffic websites are running Spark–in particular, Spark Streaming which is ideal for structuring real-time data on the fly–to reliably capture and process event data, and write it in a format that can immediately be queried by analysts.\n\nAs the world’s premiere visual bookmarking tool, Pinterest is one of the innovative organizations taking advantage of Spark. Pinterest found a natural fit in [MemSQL](https://www.memsql.com/)’s in-memory database and Spark Streaming, and is using these tools to find patterns in high-value user engagement data.\n\n### Pinterest’s Spark Streaming Setup\n##### Here’s how it works:\n\n- Pinterest pushes event data, such as pins and repins, to Apache Kafka.\n- Spark Streaming ingests event data from Apache Kafka, then filters by event type and enriches each event with full pin and geo-location data.\n- Using the [MemSQL Spark Connector](https://www.memsql.com/blog/operationalizing-spark-with-memsql/), data is then written to MemSQL with each event type flowing into a separate table. MemSQL handles record deduplication (Kafka’s “at least once” semantics guarantee fault tolerance but not uniqueness).\n- As data is streaming in, Pinterest is able to run queries in MemSQL to generate engagement metrics and report on various event data like pins, repins, comments and logins.\n\n### Visualizing the Data\nWe built a demo with Pinterest to showcase the locations of repins as they happen. When an image is repinned, circles on the globe expand, providing a visual representation of the concentration of repins by location.\n\n![Spark Demo Static](https://storage.googleapis.com/eklhad-web-public/images/spark-demo-static.gif)\n\nThe demo also leverages Spark to enrich streaming data with geolocation information between the time that it arrives and when it hits the database. MemSQL adds to this enrichment process by serving as a key/value cache for data that has already been processed and can be reused for future repins. Additionally, as part of the enrichment process, any repin that enters the system is looked up against MemSQL, and is saved to MemSQL if the full pin is missing. All full pins that come in through the stream are saved automatically to avoid this lookup.\n\n### So, What’s the Point?\nThis infrastructure gives Pinterest the ability to identify (and react to) developing trends as they happen. In turn, Pinterest and their partners can get a better understanding of user behavior and provide more value to the Pinner community. Because everything SQL based, access to data is more widespread. Engineers and analyst can work with familiar tools to run queries and track high-value user activity such as repins.\n\nIt also should be noted that this Spark initiative is just the beginning for Pinterest. As the Spark framework evolves and the community continues to grow, Pinterest expects to expand use cases for MemSQL and Spark.\n\n### Initial Results\n\nAfter integrating Spark Streaming and MemSQL, running on AWS, into their data stack, Pinterest now has a source of record for sharing relevant user engagement data and metrics their data analyst and with key brands.\n\nWith MemSQL and Spark, Pinterest has found a method for repeatable, production-ready streaming and is able to go from pipe dump to graph in real time."
 },
 {
  "id": "https://spreadsheets.google.com/feeds/list/1Ex7AuwS25FoyP_h_HYVQjKr3XwPurZd2Heos3zb2gBI/o3dkf67/public/values/clrrx",
  "name": "Workshop: Wireframing Tools and Techniques w/ Carolyn Chandler",
  "timestamp": 1384387200,
  "url": "https://gist.github.com/dahlke/7452852",
  "path": "2013-11-14-workshop-wireframing-tools-and-techniques-w-carolyn-chandler",
  "content": "# [2013-11-14] Workshop: Wireframing Tools and Techniques w/ [Carolyn Chandler](https://twitter.com/chanan)\n\n![A Design Process](https://storage.googleapis.com/eklhad-web-public/images/a_design_process.png)\n\n_**What is a wireframe?** - a visual guide that represents the skeletal framework of a website. Wireframes are created for the purpose of arranging elements to best accomplish a particular purpose._\n\nCapture ideas and sketch lo-fi interfaces. At this stage you are thinking about what _features_ not whether or not we should have a radio button or a dropdown. The lo-fi interface is useful because people are more inclined to give **honest** feedback. They are less aprehensive to make comments about a prototype than they something that looks as though someone has put a lot of effort into polishing.\n\n**Do not go down the color and typography rabit hole right at the start.**\n\n\n# Why Wireframe?\n\n- Don't let \"perfect\" be the enemy of the GOOD.\n- Don't let the \"good enough\" be the enemy of the GOOD (or the great).\n- Strong foundations\n  - _What do we need before we start developing?_\n- Common vocabulary\n  - _How do we speak about experience design details?_\n- Valued checkpoints\n  - _When does \"good enough\" become \"good\" or \"great\"?_\n- Owned by the WHOLE team, guided by the designers\n\nWireframes only communicate **partial** information to the developers.\n\n![The Process at Large](https://storage.googleapis.com/eklhad-web-public/images/the_process_at_large.gif)\n\n### Key Benefits\n- Communicate\n  - Develop a shared understanding of site layout and functionality\n  - Define page requirements\n  - Focus project team on critical elements\n  - Bring up questions the team hasn't thought of\n  - Explore user behavior and understanding (via usability testing - although prototypes are better)\n- Remote\n- Thinking (Visually)\n- Streamline the process\n\n_What is a common pattern that frustrates people? Are we doing something that potentially makes someone feel stupid just to meet the requirement?_ (Ex: GMail asking if you want to send an e-mail with no attachment after you mention that attachment in the email.)\n\n### Tools\n- Axure\n- Balsamiq\n- Pencil and Paper\n- PowerPoint\n- Keynote\n- MockFlow\n- PhotoShop\n- Illustrator\n- InDesign\n- Visio\n- OmniGraffle\n- more...\n\nStart with a low detail wireframe, and work your way through medium and high, testing all the way. You may not always make it to the high detail phase, if you are able to hand it off to your developers before hand.\n\n**Do not put real content in your Wireframes, because people tend to focus on the content then and not the design.**\n\nWireframes help to communicate page layout, information priority, and content, and behavior relationships.\n\nWireframes do **not** communicate:\n- Look and Feel\n- Treatment of buttons and/or graphic elements\n- Exact copy or verbiage\n\n### Partial Views\n\n- Allows you to hone in on single elements or portion of the page without trying to understand that piece of the project within the context of the entire project.\n\n### Storyboarding\n\n- Tell the story of the user.\n- _Why is somebody trying to do this in the first place?_\n\n\n### Before and After\n\n![Before and After](https://storage.googleapis.com/eklhad-web-public/images/before_and_after.jpeg)\n\n**Engage the visual designer earlier in the process.** Do not just hand them off a stack of wireframes. Work with them, so that they are not locked in to your layout choices, etc. You can use color for things like grouping, but try not to use too much. Being prescriptive doesn't provide the best solution.\n\n\n# How do you create a wireframe?\n\n### How will information be arranged?\n\nCreate content groupings of similar information types that are likely to be used together. THis will help users when they are searching for information.\n\n**Consider chunks**. The parts that need to be on every page (content, primary nav, utility nav, site logo, image, news, left nav, footer, etc). Add callouts and and annotations to these chunks about what you think might work for that chunk.\n\nDon't forget that you can always be sketching as you are designing.\n\nGood wireframes will achieve a balance between being too detailed and too vague.\n\n### What do you need to know before you create a wireframe?\n- Business requirements\n- Content requirements\n- Existing brand and style requirements\n- Technical requirements\n- User insight (personas, mental models, usability testing, results, etc)\n  - Anti-personas someone who would be very vocal, but might not even be a user of your system.\n\n_How will users navigate around the site?_ Determine this early.\n\n+What information should be included in a wireframe? Content, layout, behavior?_\n\n**Primary Stakeholders** - _How have our business goals been met?_\n**Developers** - _What do I have to support ad how does the site work?_\n**Visual Designers** - _What visual elements need to be displayed on the pagte?_\n**Copywriters** - _What do I need to write?_\n**Wireframe Creators** - _Why did I make these deisions?_\n\n##### Terms that are good to know\n- Elements\n- Modules/Widgets\n- Columnar Grid (960)\n- Margins\n- Padding\n\n##### How many pages do I create?\n- Home page\n- Secondary page templates (as many as needed)\n- Unique pages (search or login)\n- Process pages (registration or check out)"
 },
 {
  "id": "https://spreadsheets.google.com/feeds/list/1Ex7AuwS25FoyP_h_HYVQjKr3XwPurZd2Heos3zb2gBI/o3dkf67/public/values/cyevm",
  "name": "Election 2016: Analyzing Real-Time Twitter Sentiment with MemSQL Pipelines",
  "timestamp": 1476748800,
  "url": "http://blog.memsql.com/election-2016-real-time-twitter-sentiment/",
  "path": "2016-10-18-election-2016-analyzing-real-time-twitter-sentiment-with-memsql-pipelines",
  "content": "# Election 2016: Analyzing Real-Time Twitter Sentiment with MemSQL Pipelines\n_Originally published on the [MemSQL Blog](http://blog.memsql.com/election-2016-real-time-twitter-sentiment/)._\n\nNovember is nearly upon us, with the spotlight on Election 2016. This election has been amplified by millions of digital touchpoints. In particular, Twitter has risen in popularity as a forum for voicing individual opinions as well as tracking statements directly from the candidates. [Pew Research Center states](https://www.journalism.org/2016/07/18/candidates-differ-in-their-use-of-social-media-to-connect-with-the-public/) that “In January 2016, 44% of U.S. adults reported having learned about the 2016 presidential election in the past week from social media, outpacing both local and national print newspapers.” The first 2016 Presidential [debate](https://www.hollywoodreporter.com/news/first-presidential-debate-breaks-twitter-932779) “between Donald Trump and Hillary Clinton was the most-tweeted debate ever. All told, there were 17.1 million interactions on Twitter about the event.”\n\n---\n\nBy now, most people have probably seen both encouraging and deprecating tweets about two candidates: Hillary Clinton and Donald Trump. Twitter has become a real-time voice for the public watching along with debates and campaign announcements. We wanted to hone in on the sentiments expressed in real time. Using Apache Kafka, MemSQL, Machine Learning and our Pipelines Twitter Demo as a base, we are bringing real-time analytics to Election 2016.\n\n![Hilary vs. Trump Sentiment](https://storage.googleapis.com/eklhad-web-public/images/hillary-vs-trump-twitter-sentiment.png)\n\n~~Click here to access the live demo.~~\n\n~~Post-Election, we have shut down the demo. View a screencap of it running at the bottom of this post.~~\n\nIntroducing our latest live demonstration, Election 2016: Real-Time Twitter Analytics. We analyze the sentiment –attitude, emotion, or feeling– of every tweet about Clinton and Trump as it is tweeted. Now, anyone can see how high or low in the negative or positive tweets are trending at any given point. We’re giving everyone access to the broader scope of how each candidate is doing according to the Twittersphere.\n\n### How it Works\nFirst, we wrote a python script to collect tweets and retweets that contain the words Hillary, hillary, Trump, or trump directly from Twitter.com. We picked the words “Hillary” and “Trump” as descriptors since they are the most used for the candidates. The script pushes this content to an Apache Kafka queue in real time. Messages in this Kafka queue are then streamed using MemSQL Pipelines. Released in September 2016 at Strata+Hadoop World, Pipelines features a brand new SQL command `CREATE PIPELINE`, enabling native ingest from Apache Kafka and creation of real-time streaming pipelines.\n\nThe `CREATE PIPELINE` statement looks like this:\n\n```\nCREATE PIPELINE `twitter_pipeline`\nAS LOAD DATA KAFKA ‘your-kafka-host-ip:9092/your-kafka-topic’\nINTO TABLE `tweets`\n```\n\nThe `CREATE TABLE` statement for the `tweets` table in MemSQL is shown below:\n\n```\nCREATE TABLE `tweets` (\n`id` bigint(20) DEFAULT NULL,\n`ts` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,\n`tweet` JSON COLLATE utf8_bin,\n`text` as tweet::$text PERSISTED text CHARACTER SET utf8 COLLATE utf8_general_ci,\n`retweet_count` as tweet::%retweet_count PERSISTED int(11),\n`candidate` as CASE\nWHEN (text LIKE '%illary%') THEN 'Clinton'\nWHEN (text LIKE '%rump%') THEN 'Trump'\nELSE 'Unknown' END PERSISTED text CHARACTER SET utf8 COLLATE utf8_general_ci,\n`created` as FROM_UNIXTIME(`tweet`::$created_at) PERSISTED datetime,\nKEY `id` (`id`) /*!90619 USING CLUSTERED COLUMNSTORE */,\n/*!90618 SHARD */ KEY `id_2` (`id`)\n)\n```\n\n_Note: we create `tweets` as a columnstore table so it can handle large amounts of data for analytics. We also utilize persisted computed columns in MemSQL to parse JSON data for categorizing each tweet by candidate. MemSQL natively supports the JSON data format.\n\nWhen the `twitter_pipeline` is run, data in the `tweets` table looks like this:\n\n```\nmemsql\u003e SELECT * from tweets LIMIT 1\\G\n*************************** 1. row ***************************\nid: 786409507039485952\nts: 2016-10-13 03:33:53\ntweet: {\"created_at\":1476329611,\"favorite_count\":0,\"id\":786409507039485952,\"retweet_count\":0,\"text\":\"RT @BlackWomen4Bern: This will be an interesting Halloween this year...expect me to tweet some epic Hillary costumes...I expect there will…\",\"username\":\"hankandmya12\"}\ntext: RT @BlackWomen4Bern: This will be an interesting Halloween this year...expect me to tweet some epic Hillary costumes...I expect there will…\nretweet_count: 0\ncandidate: Clinton\ncreated: 2016-10-13 03:33:31\n1 row in set (0.03 sec)\n```\n\nNext, we created a second pipeline that pulled from the same Kafka topic, but instead of storing directly into a table, we perform real-time sentiment analysis with a MemSQL Pipelines transform that leverages the Python Natural Language Toolkit ([nltk](http://www.nltk.org/)) Vader module. The `CREATE PIPELINE` statement for the second pipeline looks like this:\n\n```\nCREATE PIPELINE `twitter_sentiment_pipeline`\nAS LOAD DATA KAFKA 'your-kafka-host-ip:9092/your-kafka-topic'\nWITH TRANSFORM ('http://download.memsql.com/pipelines-twitter-demo/transform.tar.gz' , 'transform.py' , '')\nINTO TABLE `tweet_sentiment`\n```\n\nCombining data from these two MemSQL pipelines, we can perform analytics using SQL. For example, we can create a histogram of tweet sentiment through the following query:\n\n```\nSELECT\nsentiment_bucket,\nSUM(IF(candidate = \"Clinton\", tweet_volume, 0)) as clinton_tweets,\nSUM(IF(candidate = \"Trump\", tweet_volume, 0)) as trump_tweets\nFROM tweets_per_sentiment_per_candidate_timeseries t\nGROUP BY sentiment_bucket\nORDER BY sentiment_bucket;\n```\n\nLastly we constructed a User Interface (UI). We built the graph using WebSockets and React to visualize the rolling average tweet sentiment for both candidates, drawn in real time.\n\n~~Click here to access the live demo.~~\n\n~~Post-Election, we have shut down the demo. View a screencap of it running at the bottom of this post.~~\n\n![Real Time Sentiment Graph Screenshot](https://storage.googleapis.com/eklhad-web-public/images/real-time-twitter-sentiment-election-2016.jpg)\n"
 },
 {
  "id": "https://spreadsheets.google.com/feeds/list/1Ex7AuwS25FoyP_h_HYVQjKr3XwPurZd2Heos3zb2gBI/o3dkf67/public/values/cztg3",
  "name": "Setting up a Raspberry Pi web server using Node \u0026 Express",
  "timestamp": 1383609600,
  "url": "https://gist.github.com/dahlke/7330365",
  "path": "2013-11-05-setting-up-a-raspberry-pi-web-server-using-node-express",
  "content": "# [2013-11-05] Setting up a Raspberry Pi web server using Node \u0026 Express\n\nWhat you'll need:\n\n- At the start\n  - USB Keyboard\n  - Monitor\n  - Ethernet connection\n  - HDMI Cable or Video Composite Cable\n- Always:\n  -   Raspberry Pi\n  -   Power Source\n  -   4GB SD Card\n  -   GitHub Account\n- Optional:\n  -   A computer for SSH access, and formatting the SD card (OSX assumed for this tutorial)\n  -   USB WiFi Dongle (or not if you stick with Ethernet)\n\n\n### Installing Raspian\n\nThe first thing that you're going to need to get going on your Pi is an operating system, and for this walkthrough I'm using Raspian, a popular Raspberry Pi specific flavour of Debian. To start, you'll need to [download the latest .zip version of Raspian](http://www.raspberrypi.org/downloads). Once you have the file, you'll want to unzip it, revealing the OS image.\n\nNow we need to write the file to the SD card, but before you do, enter the following command into a terminal window.\n\n```\ndf -h\n```\n\nYou should see something like the following for output:\n\n```\nFilesystem      Size   Used  Avail  Capacity  Mounted on\n/dev/disk0s2   465Gi  117Gi  348Gi    26%     /\ndevfs          193Ki  193Ki    0Bi   100%     /dev\nmap -hosts       0Bi    0Bi    0Bi   100%     /net\nmap auto_home    0Bi    0Bi    0Bi   100%     /home\n```\n\nAt this point you're going to need your SD card. Insert the card into the SD slot and enter `df -h` into the console again.\n\nThis time you will see something more along the lines of\n\n```\nFilesystem      Size   Used  Avail  Capacity  Mounted on\n/dev/disk0s2   465Gi  117Gi  348Gi    26%     /\ndevfs          193Ki  193Ki    0Bi   100%     /dev\nmap -hosts       0Bi    0Bi    0Bi   100%     /net\nmap auto_home    0Bi    0Bi    0Bi   100%     /home\n/dev/disk1s1    15Gi  1.1Mi   15Gi     1%     /Volumes/SD\n```\n\nThe mount `/Volumes/SD` is our target, you'll want to unmount it using the terminal command `diskutil unmount /dev/disk1s1`.\n\n\n\nTo copy the new image of Raspian to your SD card enter the following command. It contains the path to your SD, however, the `s1` is dropped off, and an `r` is appended to the front, producing `rdisk1`.\n\n__Be _very_ careful when doing this, as if you write to the wrong disk, you could end up copying the image to your computer's hard drive.__\n```\nsudo dd if=2013-09-25-wheezy-raspbian.img of=/dev/rdisk1 bs=1m\n```\n\nThis command will run silently for some time, and will then give you a little bit of output based on it's success or failure. This may take a while so be patient.\n\nWith the disk re-imaged, you are now free to eject the SD card using `diskutil eject /dev/rdisk1`. Once you do, go ahead and plug it into your Raspberry Pi. You'll also want to plug your keyboard into the Pi at this time. Once you have both inserted securely, go ahead and plug the power source into your Raspberry Pi. You should see a typical Linux boot screen, and will have the option to make any configurations to your Pi that you might want to make. When the Pi is finally done booting, you'll be prompted with:\n\n```\nrasberrypi login:\n```\n\nTo get past this screen use the user `pi` with the password: `raspberry`.\n\n### Setting up SSH\n[source](http://cplus.about.com/od/raspberrypi/a/How-Do-I-Setup-Ssh-On-Raspberry-Pi.htm)\n\nAnd we're in. But we don't want to be working on a keyboard plugged directly into the device the whole time, so we're going to enable SSH on the Pi. First, we need to make sure that we have the necessary libraries by running `sudo apt-get install ssh`.\n\n```\nsudo /etc/init.d/ssh start\n```\n\nTo have this happen automatically on bootup, use:\n\n```\nsudo update-rc.d ssh defaults\n```\nThen check to see that it worked using:\n```\nsudo reboot\n```\n\nThis is especially nice when you don't want to pull out a keyboard every time you fire up the Pi. It will auto start it's SSH listener, and you can tunnel in without needing a keyboard or monitor on the device. To shut down the Pi in general, use `sudo shutdown -h now`. You can swap out the option `-h` with `-r` and it will perform the same action as `sudo reboot`.\n\nThen we'll want to check your ip address. To do this, in the shell enter `/sbin/ifconfig`. The resulting inet addr for eth0 represents your IP address. Once we have the IP address, we can attempt to SSH into the Pi. To do this on in OSX, simply use `ssh pi@1.1.1.1` with `1.1.1.1` replaced by your IP address. When prompted for your password, enter `raspberry`, and you're in.\n\n\n### Setting up WiFi\n[source](http://learn.adafruit.com/adafruits-raspberry-pi-lesson-3-network-setup/setting-up-wifi-with-occidentalis)\n\nNow we want to set up our WiFi dongle with the device so that we don't have to plug it in via ethernet every time we want to use it. To scan for wifi networks, run `iwlist scan`. Once you locate the network you want,open up your interfaces config by using `sudo nano /etc/network/interfaces`. Edit the file to look like the following:\n\n```\nauto lo\n\niface lo inet loopback\niface eth0 inet dhcp\n\nallow-hotplug wlan0\nauto wlan0\n\n\niface wlan0 inet dhcp\n        wpa-ssid \"NETWORK_NAME\"\n        wpa-psk \"NETWORK_PASSWORD\"\n```\n\n### Installing NodeJS\n[source](http://oskarhane.com/raspberry-pi-install-node-js-and-npm/)\n\nFirst of all we need to create a place in the file system for Node to live, we'll do that using `sudo mkdir /opt/node`. Once it's created, we can fetch version of Node that we want to run in our system with `wget http://nodejs.org/dist/v0.10.4/node-v0.10.8-linux-arm-pi.tar.gz`. Unzip the file using `tar xvzf node-v0.10.8-linux-arm-pi.tar.gz`. Move all the files that have been unzipped to the new home we created for them with `sudo cp -r node-v0.10.8-linux-arm-pi/* /opt/node`. Head back to the root directory (`cd ~`) and edit your bash profile with `nano .bash_profile`. Add the following.\n\n```\nPATH=$PATH:/opt/node/bin\nexport PATH\n```\n\nThen run `source .bash_profile`. To ensure that everything installed properly, we can use `node -v` and `npm -v` and get the version numbers for Node and the package manager. Now feel free to remove the tarball and uncompressed files we used earlier from your SD card.\n\n### Setting up your Pi with GitHub\n[source](https://help.github.com/articles/generating-ssh-keys#platform-linux)\n\nNow we've got Node, and we're going to want to be able to pull in projects to run on top of it. To do this, we're going to register our Pi device with the GitHub account we want to pull from. First, we need to generate an SSH key. To do this, run `ssh-keygen -t rsa -C \"YOUR_EMAIL@EXAMPLE.COM\"`. You will then be prompted for a file name, just hit enter. You will then be asked to give your ssh key a password. Enter your password twice, and your key will be generated. The contents of the key will be stored at `~/.ssh/id_rsa.pub`. Open the file and copy the contents to your clipboard, then add it to your GitHub account through the GitHub web interface using _Account Settings \u003e SSH Keys \u003e Add SSH Key_. Once you have add your key, you should be able to clone any of your git repositories. For example, enter the directory you would like to clone the project into and use `git@github.com:neildahlke/pi-sockets.git` to clone the `pi-sockets` example we will be using below. With the project cloned, go ahead and run `node app.js` in the new project directory, then visit the URL you specified earlier on port 8888.\n\nYour Raspberry Pi is now an active web server.\n\n\n### Backing up your Pi\n[source](http://raspberrypi.stackexchange.com/questions/311/how-do-i-backup-my-raspberry-pi)\n\nThere is nothing worse than completing all of these steps, then having power supply issues corrupt your SD card. To avoid the long hassle of setting up your Pi again, it is recommended that you back up your SD card at this point. To do that, take the SD card with the properly set up image, and server code, and plug it into your Mac. Once it's in, run the following:\n\n```\ndd if=/dev/rdiskx of=/path/to/image bs=1m\n```\n\nWhere `/dev/rdiskx` is your SD card and the `x` is the number corresponding to your SD."
 },
 {
  "id": "https://spreadsheets.google.com/feeds/list/1Ex7AuwS25FoyP_h_HYVQjKr3XwPurZd2Heos3zb2gBI/o3dkf67/public/values/d180g",
  "name": "Migrating A Lot of State with Python and the Terraform Cloud API",
  "timestamp": 1586476800,
  "url": "https://medium.com/hashicorp-engineering/migrating-a-lot-of-state-with-python-and-the-terraform-cloud-api-997ec798cd11",
  "path": "2020-04-10-migrating-a-lot-of-state-with-python-and-the-terraform-cloud-api",
  "content": "# Migrating _A Lot_ of State with Python and the Terraform Cloud API\n_Originally published on the [HashiCorp Solutions Engineering Blog](https://medium.com/hashicorp-engineering/migrating-a-lot-of-state-with-python-and-the-terraform-cloud-api-997ec798cd11)._\n\n\n### Overview\nOne of the first questions I hear in the field after explaining the merits of [Terraform Cloud](https://www.terraform.io/docs/cloud/index.html) (TFC) is: how hard is it to migrate all my existing state? This is a fair question. When you have built up years worth of Terraform configuration and state over time, it can be a daunting proposition to consider migrating it all to one centralized platform. That is, if you haven’t read this blog post.\n\nFor those who may not yet be familiar with it, TFC is an application that allows teams to use Terraform together in a consistent and reliable environment with access controls and reliable state management features. HashiCorp also provides a self-hosted distribution of TFC, called [Terraform Enterprise](https://www.terraform.io/docs/enterprise/index.html) (TFE).\n\nIn TFC and TFE, infrastructure is organized into [Workspaces](https://www.terraform.io/docs/cloud/workspaces/index.html). Each TFC workspace is typically linked to a VCS repository (which contains the Terraform configuration) and has its own associated metadata (variable values, credentials, and secrets).\n\nIf you’ve used Terraform OSS in the past, there’s no doubt you’re familiar with the [State Files](https://www.terraform.io/docs/state/index.html) that Terraform configurations have. Each TFC workspace also has its own state file. When you’re planning for a migration to TFC or TFE, you can project that the number of Terraform OSS state files you currently have will map roughly one to one to the number of TFC workspaces you will need.\n\n*Note*: TFC workspaces are different from [Terraform OSS workspaces](https://www.terraform.io/docs/state/workspaces.html), which allow a single Terraform OSS configuration to have multiple state files. Each TFC workspace is associated with a single “default” Terraform OSS workspace and only has a single state file. So, if you have a Terraform configuration that uses 3 Terraform OSS workspaces with 3 corresponding state files, you will migrate that configuration to 3 TFC workspaces. Finally, if your existing Terraform configurations use the [current workspace interpolation](https://www.terraform.io/docs/state/workspaces.html#current-workspace-interpolation), `${terraform.workspace}`, you will need to remove it because it will always evaluate to “default” in TFC.\n\nThe above discussion of workspaces and state files leads us to a big question: What do I do if I have a _lot_ of state files?\n\nWhile there are some good recommendations in the TFC docs for [Migrating State from Local Terraform](https://www.terraform.io/docs/cloud/migrate/index.html) to [Terraform Cloud and Migrating State from Multiple Local Workspaces](https://www.terraform.io/docs/cloud/migrate/index.html), the methodology documented there could be burdensome for a large number of state files. However, with a little creativity and the help of the [TFC API](https://www.terraform.io/docs/cloud/api/index.html), we can automate this painful process away.\n\n### Challenge\nConsider the following challenge: your team has been leveraging the [GCS backend](https://www.terraform.io/docs/backends/types/gcs.html) with Terraform OSS and has a bunch of state files in a GCS bucket. You’ve already either [signed up for a Terraform Cloud account](https://www.terraform.io/docs/cloud/getting-started/access.html) or stood up your own [Terraform Enterprise deployment](https://www.terraform.io/docs/enterprise/index.html). You have also already [connected your VCS provider](https://www.terraform.io/docs/cloud/vcs/index.html). You want to migrate entirely to TFC, but you don’t want to spend hours updating the backend configuration in each of the workspaces and migrating the state files one at a time.\n\n_*Note*: You’ll be using Python in this example, and will lean on the [Google Cloud Storage Python library](https://cloud.google.com/storage/docs/reference/libraries#client-libraries-install-python) and [`terrasnek`](https://github.com/dahlke/terrasnek), a Python library for the TFC API. While all of the code below is Python, the concepts can be translated to any language you want and there are other client libraries and tools available [here](https://www.terraform.io/docs/cloud/api/index.html#client-libraries-and-tools)._\n\nYou’ve created a GCS bucket called `hc-neil` (great choice!) with three state files in it. Each of them will eventually be mappped to a TFC VCS repository in your connected VCS provider. You might have many more than three state files in a bucket. While this example focuses on GCP, you can easily translate all of this work to another cloud provider like AWS or Azure.\n\nIf you use the [`gsutil`](https://cloud.google.com/storage/docs/gsutil_install#mac) CLI tool to traverse the bucket, you’ll see something like the following output.\n\n```\ngs://hc-neil/demo-tfstates/:\ngs://hc-neil/demo-tfstates/\ngs://hc-neil/demo-tfstates/demo-project-aws/:\ngs://hc-neil/demo-tfstates/demo-project-aws/\ngs://hc-neil/demo-tfstates/demo-project-aws/two-tier-demo-app-aws.tfstate\ngs://hc-neil/demo-tfstates/demo-project-azure/:\ngs://hc-neil/demo-tfstates/demo-project-azure/\ngs://hc-neil/demo-tfstates/demo-project-azure/two-tier-demo-app-azure.tfstate\ngs://hc-neil/demo-tfstates/demo-project-gcp/:\ngs://hc-neil/demo-tfstates/demo-project-gcp/\ngs://hc-neil/demo-tfstates/demo-project-gcp/two-tier-demo-app-gcp.tfstate\n```\n\nThat output is helpful in telling you where all the state files live; determining that is the first step in mapping them over to TFC. You’ll also want to take a look at your [VCS repository](https://github.com/dahlke/tfc-blog-example)’s layout, so that you understand which Terraform configuration code you will eventually map to each state file.\n\n```\n./two-tier-tfc-demo-app:\naws azure gcp\n./two-tier-tfc-demo-app/aws:\nmain.tf outputs.tf terraform.tfvars variables.tf versions.tf\n./two-tier-tfc-demo-app/azure:\nmain.tf outputs.tf terraform.tfvars variables.tf\n./two-tier-tfc-demo-app/gcp:\nmain.tf outputs.tf terraform.tfvars variables.tf versions.tf\n```\n\n### The Approach\nKnowing both the state file layout in GCS and the Terraform Configuration layout in your VCS repository, you can start to formulate a mental model of how they will be mapped to each other.\n\n```\n./two-tier-tfc-demo-app/aws → gs://hc-neil/demo-tfstates/demo-project-aws/\n./two-tier-tfc-demo-app/azure → gs://hc-neil/demo-tfstates/demo-project-azure/\n./two-tier-tfc-demo-app/gcp → gs://hc-neil/demo-tfstates/demo-project-gcp/\n```\n\n![Mapping Buckets](https://storage.googleapis.com/eklhad-web-public/images/mapping-buckets.png)\n_Mapping GCS buckets to VCS repositories, illustrated_\n\nOnce you understand how they map to each other, you can begin the automation process by defining a migration strategy, implemented in this case in a JSON file so that it is machine-readable. Let’s call it the “Migration Map”. You’ll also add a couple other pieces of data to the migration map, including the branch of the VCS repository you want to use, the Terraform version to use, and the name you want to give to the workspace.\n\n```\n[\n    {\n        \"gcs-blob-path\": \"demo-tfstates/demo-project-aws/two-tier-demo-app-aws.tfstate\",\n        \"statefile-local-path\": null,\n        \"repo\": \"dahlke/tfc-blog-example\",\n        \"working-dir\": \"AWS-two-tier-tfc-demo-app/aws\",\n        \"workspace-name\": \"AWS-gcs-to-tfc-demo\",\n        \"branch\": \"master\",\n        \"tf-version\": \"0.12.1\"\n    },\n    …\n]\n```\n\nIn the end, you should have one of these JSON objects for each state file you plan to migrate. With this migration strategy defined in a machine readable format, you can begin prepping your script for migration.\n\n### Migration Prep\n##### Step 1: Configure Environment for Google Compute Engine\nIn order to make requests against the GCP API using the Google Cloud Storage Python library, you will need to authenticate. There are some instructions for setting up authentication here. Make sure that you follow these instructions and set the `GOOGLE_APPLICATION_CREDENTIALS` environment variable properly for your OS.\n\n```\nexport GOOGLE_APPLICATION_CREDENTIALS=”/Users/neil/.gcp/hashi/foo.json”\n```\n\n_Warning: Do NOT store these credentials in VCS._\n\nYou’ll also have to use an environment variable to tell the script which GCS bucket to read from.\n\n```\nexport GCS_BUCKET_NAME=”hc-neil”\n```\n\n##### Step 2: Configure Environment for TFC\n\nOnce you have programmatic access to the GCS bucket with your state files in it, you need to configure authentication for programmatic TFC API access.\nTerraform Enterprise has multiple different types of tokens (User, Team and Organization) which come with different [access levels](https://www.terraform.io/docs/cloud/users-teams-organizations/api-tokens.html#access-levels). For simplicity, let’s assume that you are a TFC admin and are using a [User Token](https://www.terraform.io/docs/cloud/users-teams-organizations/users.html#api-tokens). Follow the instructions to generate a new user token for yourself and then set it as the `TFC_TOKEN` environment variable.\n\n```\nexport TFC_TOKEN=”YOUR_TFC_USER_TOKEN”\n```\n\n_Warning: Do NOT store this token in VCS._\n\nSince you’re going to connect each TFC workspace you create to a VCS repository, you’ll need to get the OAuth Token ID for your VCS from TFC. This is simple: Go to your organization settings, choose VCS Providers, and then copy the OAuth Token ID you want to use. Then export it.\n\n```\nexport TFC_OAUTH_TOKEN_ID=”YOUR_TFC_OAUTH_TOKEN”\n```\n\nYou’ll also need to tell the script where your TFC or TFE instance lives, and what organization to use. For example, if you were using HashiCorp’s managed deployment of Terraform Cloud, you would use [`https://app.terraform.io`](https://app.terraform.io).\n\n```\nexport TFC_ORG=\"YOUR_TFC_ORG\"\nexport TFC_URL=\"YOUR_TFC_URL\"\n```\n\n##### Step 3: Install the Python Dependencies\nSince this example uses Python, you need to install some dependencies to make writing our script a little easier.\n\n```\npip3 install google-cloud-storage==1.26.0\npip3 install terrasnek==0.0.2\n```\n\nWith all the pieces in place, you can execute the migration.\n\n### Executing the Migration\n\n![Migration Workflow](https://storage.googleapis.com/eklhad-web-public/images/migration-workflow.png)\n_Migration workflow, illustrated_\n\nYou can see the workflow above. Below, you’ll walk through each component. Since the focus in this blog is to help you understand the individual steps, we will focus on those. If you want to see the full script that you can use for automation, it is available [here](https://github.com/dahlke/migrate-to-tfc/blob/master/main.py).\n\n##### Step 1: Runtime Configuration\nThe first thing to do is read in all the variables your script will need from the environment. You’ll notice that the GCP credentials are not read in explicitly since that is done by the GCS library itself. You also should read in the Migration map from your JSON file. Once all the configuration is loaded into the script, you can create the GCS and TFC clients.\n\n```\nTFC_TOKEN = os.getenv(\"TFC_TOKEN\", None)\nTFC_URL = os.getenv(\"TFC_URL\", \"https://app.terraform.io\")\nTFC_ORG = os.getenv(\"TFC_ORG\", None)\nGCS_BUCKET_NAME = os.getenv(\"GCS_BUCKET_NAME\", None)\n\nmigration_targets = []\n\n# Read the Migration Map from the file system into a Python dict\nwith open(\"migration.json\", \"r\") as f:\n    migration_targets = json.loads(f.read())\n\n# Create the GCS client\nstorage_client = storage.Client()\n\n# Create a Terraform Enterprise client with the TFC_TOKEN from the\n# environment\napi = TFC(TFC_TOKEN, url=TFC_URL)\n\n# Set the organization to work in for our client\napi.set_organization(TFC_ORG)\n```\n\nNow, the script is ready to start pulling down state files and pushing them up to TFC.\n\n##### Step 2: Retrieve the Latest State Files and Save Locally\nOnce authenticated, you’ll pull down the state file from each specified GCS blob path.\n\n_Warning: This demo does not account for users accessing GCS while the migration is being performed. If this is a concern, be sure to lock your state before proceeding._\n\n```\n# Connect to the bucket we want to download blobs from\nbucket = storage_client.bucket(GCS_BUCKET_NAME)\n\n# Retrieve the path from the migration target dict\nblob_path = mt['gcs-blob-path']\n\n# Create a blob object\nblob = bucket.blob(blob_path)\n\n# Extract the state file name from the blob and use\n# it to define the path we want to save the state file\n# locally\nstatefile_name = blob.name.split(\"/\")[-1]\nstatefile_path = f\"statefiles/{statefile_name}\"\n\n# Download the state file to the local path just defined\nblob.download_to_filename(statefile_path)\n\n# Add the local path we saved the state file to\n# in the migration targets dict for usage later\nmt[\"statefile-local-path\"] = statefile_path\n```\n\n##### Step 3: Create a TFC Workspace\nUp until this point, you have not used the TFC API at all. Let’s change that. The first thing you’ll need to do in TFC is create a workspace, so you’ll want to reference the [Workspace API docs](https://www.terraform.io/docs/cloud/api/workspaces.html#create-a-workspace) to do that. Target the “With a VCS Repository” sample payload and then start building your own.\n\n```\n# Configure our create payload with the data\n# from the migration targets JSON file\ncreate_ws_payload = {\n    \"data\": {\n        \"attributes\": {\n            \"name\": mt[\"workspace-name\"],\n            \"terraform_version\": mt[\"tf-version\"],\n            \"working-directory\": mt[\"working-dir\"],\n            \"vcs-repo\": {\n                \"identifier\": mt[\"repo\"],\n                \"oauth-token-id\": oauth_client_id,\n                \"branch\": mt[\"branch\"],\n                \"default-branch\": True\n            }\n        },\n        \"type\": \"workspaces\"\n    }\n}\n\n# Create a workspace with the VCS attached\nws = api.workspaces.create(create_ws_payload)\n\n# Save the workspace ID for usage when adding a state version\nws_id = ws[\"data\"][\"id\"]\n```\n\nYou can see directly how some of the keys you configured in your migration map are leveraged here. You’ll use `workspace-name`, `repo`, `tf-version`, `branch`, and `working-dir` to properly set up your workspace. You’ll also want to save the workspace ID after you create it for the next step.\n\n##### Step 4: Upload the State File as a State Version\nYour TFC workspace is now created and connected to your specified VCS repository, however, it still does not have the state you wanted to migrate over. This part can be a bit tricky. The state file you downloaded earlier is in plaintext. The State Versions API [requires](https://www.terraform.io/docs/cloud/api/state-versions.html#request-body) that the state file be encoded as a base64 string, and you need to provide an md5 hash of that so Terraform can verify the upload.\n\n```\n# Read in the state file contents we just pulled from GCS\nraw_state_bytes = None\nwith open(mt[\"statefile-local-path\"], \"rb\") as infile:\n    raw_state_bytes = infile.read()\n\n# Perform a couple operations on the data required for the\n# create payload. See more detail here:\n# https://www.terraform.io/docs/cloud/api/state-versions.html\nstate_hash = hashlib.md5()\nstate_hash.update(raw_state_bytes)\nstate_md5 = state_hash.hexdigest()\nstate_b64 = base64.b64encode(raw_state_bytes).decode(\"utf-8\")\n```\n\nNow you have massaged your state file into base64 and have created an MD5 hash of it. You can build the payload for creating a State Version. Please reference the [State Versions API docs](https://www.terraform.io/docs/cloud/api/state-versions.html).\n\n```\n# Build the payload\ncreate_state_version_payload = {\n    \"data\": {\n        \"type\": \"state-versions\",\n        \"attributes\": {\n            \"serial\": 1,\n            \"md5\": state_md5,\n            \"state\": state_b64\n        }\n    }\n}\n```\n\nThere is one last caveat: you cannot upload state to a workspace that is not locked. This is done to make sure you don’t modify state underneath an existing run. You’ll need to lock the workspace, create your state version, and then unlock the workspace.\n\n```\n# State versions cannot be modified if the workspace isn't locked\napi.workspaces.lock(ws_id, {\"reason\": \"migration script\"})\n\n# Create the state version\napi.state_versions.create(ws_id, create_state_version_payload)\n\n# Unlock the workspace so other people can use it\napi.workspaces.unlock(ws_id)\n```\n\nNow you have a complete pipeline. You have pulled state files from GCS, created new TFC workspaces to hold those state files, and uploaded those state files to your new workspaces. Congrats on an easy migration!\n\n### Conclusion\nThe TFC API provides much more value than just migration — it is the key to unlocking the true automation potential of Terraform Cloud and Terraform Enterprise. While this simple example focuses primarily on state and workspaces, you can use that same API to manage variables, trigger runs, export plan and apply logs, get results from cost estimates and Sentinel policy checks, and more. The Terraform Cloud API is a very powerful tool in your tool belt. I hope you can use this blog post as inspiration for automating your migration — get building!\n\n[Full script available in the GitHub repository.](https://github.com/dahlke/migrate-to-tfc)\n\n_Note: All of the code is available in this [repository](https://github.com/dahlke/gcs-to-tfe). While this is a GCP-based example, you can apply the same logic of extracting state files and creating TFC workspaces to your favorite cloud provider._"
 }
]